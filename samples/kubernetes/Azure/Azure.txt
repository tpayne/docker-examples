# Azure AKS commands of use...

az login
az aks install-cli

# Set up kubectl to work with AKS...
az aks get-credentials --resource-group <rgName> --name <clusterName>
az aks list --query [].name
kubectl get pods

az aks start --resource-group <rgName> --name <clusterName>
az aks stop --resource-group <rgName> --name <clusterName>

# Sample setup...
REGION_NAME=eastus
RESOURCE_GROUP=testaks
SUBNET_NAME=aks-subnet
VNET_NAME=aks-vnet

az group create --name $REGION_NAME --location $REGION_NAME
az network vnet create \
  --resource-group $RESOURCE_GROUP \
  --location $REGION_NAME     \
  --name $VNET_NAME    \
  --address-prefixes 10.0.0.0/8    \
  --subnet-name $SUBNET_NAME     \
  --subnet-prefixes 10.240.0.0/16
  
SUBNET_ID=$(az network vnet subnet show \
    --resource-group $RESOURCE_GROUP \
    --vnet-name $VNET_NAME \
    --name $SUBNET_NAME \
    --query id -o tsv)
    
AKS_CLUSTER_NAME=aksworkshop-123

# Cluster networking is either kubenet or Azure CNI
# - Kubenet is K8 default - assigns IPs to nodes from subnet, pod IPs get assigned from another address space. NAT routing is used
# - Azure CNI - assigns pod IPs from the same address space. Can access pods directly

az aks create --resource-group $RESOURCE_GROUP --name $AKS_CLUSTER_NAME \
  --vm-set-type VirtualMachineScaleSets --node-count 2 --load-balancer-sku standard \
  --location $REGION_NAME --kubernetes-version $VERSION --network-plugin azure \
  --vnet-subnet-id $SUBNET_ID --service-cidr 10.2.0.0/24 --dns-service-ip 10.2.0.10 \
  --docker-bridge-address 172.17.0.1/16 --generate-ssh-keys
  
az aks get-credentials --resource-group $RESOURCE_GROUP --name $AKS_CLUSTER_NAME

ACR_NAME=acrtest

az acr create \
  --resource-group $RESOURCE_GROUP \ 
  --location $REGION_NAME \
  --name $ACR_NAME --sku Standard
 
az acr build --resource-group $RESOURCE_GROUP --registry $ACR_NAME  \
  --image <imageName> (finds it from local Dockerfile)
  
az acr repository list --name $ACR_NAME --output table

az aks update \
  --name $AKS_CLUSTER_NAME     \
  --resource-group $RESOURCE_GROUP    \
  --attach-acr $ACR_NAME
  
# Run some helm commands (helm is advanced package manager for K8s)
# - add helm repo to list
# - search repo for examples
# - install mongodb package with some defaults
# - install secret for db to use

helm repo add bitnami https://charts.bitnami.com/bitnami
helm search repo bitnami

helm install ratings bitnami/mongodb --namespace <k8nameSpace> \
  --set auth.username=<userName>,auth.password=<passwd>,auth.database=<dbName>
  
kubectl create secret generic mongosecret --namespace <k8nameSpace>  \
  --from-literal=MONGOCONNECTION="mongodb://<userName>:<passwd>@<dbName>-mongodb.<dbName>:27017/<dbName>"
  
kubectl describe secret mongosecret --namespace <k8nameSpace>

#Â Install ingress controller
# An ingress controller is a level 7 LB - allows proxy, routing control, reverse proxy etc.
kubectl create namespace ingress
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install nginx-ingress ingress-nginx/ingress-nginx \
  --namespace ingress     \
  --set controller.replicaCount=2   \
  --set controller.nodeSelector."beta\.kubernetes\.io/os"=linux     \
  --set defaultBackend.nodeSelector."beta\.kubernetes\.io/os"=linux

# Install a cert manager
kubectl create namespace cert-manager
helm repo add jetstack https://charts.jetstack.io
helm repo update
kubectl apply --validate=false \
  -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.14/deploy/manifests/00-crds.yaml
helm install cert-manager \
  --namespace cert-manager \
  --version v0.14.0     \
  jetstack/cert-manager
  
kubectl describe cert ratings-web-cert --namespace <k8nameSpace>

WORKSPACE=aksws
az resource create --resource-type Microsoft.OperationalInsights/workspaces         \
  --name $WORKSPACE         \
  --resource-group $RESOURCE_GROUP   \
  --location $REGION_NAME   \
  --properties '{}' -o table
  
WORKSPACE_ID=$(az resource show --resource-type Microsoft.OperationalInsights/workspaces \
    --resource-group $RESOURCE_GROUP \
    --name $WORKSPACE \
    --query "id" -o tsv)
    
az aks enable-addons     \
  --resource-group $RESOURCE_GROUP     \
  --name $AKS_CLUSTER_NAME     \
  --addons monitoring     \
  --workspace-resource-id $WORKSPACE_ID
  
